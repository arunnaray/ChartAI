import openai
import streamlit as st
import re
import numpy as np
import random

def choose_random_section(lst, section_length=5):
    if len(lst) < section_length:
        raise ValueError("Section length cannot be greater than the length of the list.")
    
    start_index = random.randint(0, len(lst) - section_length)
    end_index = start_index + section_length

    return lst[start_index:end_index] 

# def gpt_promt(message):
#     response = openai.ChatCompletion.create(
#         model='gpt-3.5-turbo',
#         messages=[
#             {"role": "system", "content": "You are an actuary."},
#             {"role": "user", "content": message},
#         ])
#     return response.choices[0]['message']

def gpt_promt_davinci(message):
    response = openai.Completion.create(
        model="gpt-3.5-turbo-instruct",
        prompt=message,
        temperature=0.01,
        max_tokens=1000)
    return response.choices[0]['text']

@st.cache_data(show_spinner=False)
def create_sample_question(data, regenerate_new_question):
    data = choose_random_section(data,5)
    # goals_sample_question = re.findall(r"<goal_start>(.*)<goal_end>", regenerate_new_question.replace("\n", ' '))

    # if len(goals_sample_question) > 0:
    #     prompt = f"You are a document analyst, " \
    #              f"Generate me 10 questions based on data using the text data {data}, use " \
    #              f"please generate the questions to meet the objective of {goals_sample_question}" \
    #              f"Put each question in <question_start> your generated question <question_end>."
    # else:
    prompt = f"You are a document analyst, " \
                f"Generate me 10 questions based on data using the text data {data}, use " \
                f"Put each question in <question_start> your generated question <question_end>."

    response = gpt_promt_davinci(prompt)
    
    try:
        questions = re.findall("<question_start>(.*?)<question_end>", response.replace("\n", ' '))
        n = 5
        random_choice = random.sample(questions, k=n)
        question_1 = random_choice[0]
        question_2 = random_choice[1]
        question_3 = random_choice[2]
        question_4 = random_choice[3]
        question_5 = random_choice[4]
    except:
        question_1 = "No Questions Generated! Click Refresh Questions"
        question_2 = "No Questions Generated! Click Refresh Questions"
        question_3 = "No Questions Generated! Click Refresh Questions"
        question_4 = "No Questions Generated! Click Refresh Questions"
        question_5 = "No Questions Generated! Click Refresh Questions"

    return question_1, question_2, question_3, question_4, question_5

@st.cache_data(show_spinner=False)
def recursion_batch(list_of_df, list_of_result, new_question, query_recommendation):
    '''
    :param query_recommendation: The query that was created by GPT3 API
    :param new_question: The question that was asked by the user
    :param dataframe_new: The dataframe that was created in DuckDB with the query generated by GPT3
    :param list_of_result: Empty list to store the result from chat gpt
    :return: Recursive response from chat GPT
    '''

    # print("Recursive batch length: ", len(list_of_df[0].to_json()))
    # print("Recursive batch: ", list_of_df[0])
    # print("Length: ", len(list_of_result))
    # print("Content: ", list_of_result)
    if len(list_of_df) <= 10:
        if len(list_of_df) < 2:
            dataframe_json = list_of_df[0].to_json()
            prompt = f"You are an analyst, " \
                     f"Please give a report and insights of the result in human readable text: " \
                     f"The question '{new_question}' was asked. The result has been generated using {query_recommendation}," \
                     f"Answering in a way that answers the question, explain the result: {dataframe_json}" \
                     f"Do not show the query in the answer."
            list_of_result = list_of_result + [gpt_promt_davinci(prompt)]
            return list_of_result
        else:
            dataframe_json = list_of_df[0].to_json()
            prompt = f"You are an analyst, " \
                     f"Please give a report and insights of the result in human readable text: " \
                     f"The question '{new_question}' was asked. The result has been generated using {query_recommendation}," \
                     f"Answering in a way that answers the question, explain the result: {dataframe_json}" \
                     f"Do not show the query in the answer."
            list_of_result = list_of_result + [gpt_promt_davinci(prompt)]
            new_list = list_of_df[1:]
            return recursion_batch(new_list, list_of_result, new_question, query_recommendation)
    else:
        st.error('Performing huge data set analysis is disabled for now...')
        return "Sorry, we've disabled huge processing of large file insights for now..."

@st.cache_data(show_spinner=False)
def recursive_summarizer_sub(list_of_response, list_of_result_response, new_question):

    if len(list_of_response) < 2:
        list_of_result_response = list_of_result_response + list_of_response
        return list_of_result_response
    else:
        data = '\n'.join(list_of_response[0])
        prompt = f"Given the question is {new_question}." \
                 f"Summarize the following text after: {data}"
        list_of_result_response = list_of_result_response + [gpt_promt_davinci(prompt)]
        new_list = list_of_response[1:]
        return recursive_summarizer_sub(new_list, list_of_result_response, new_question)

@st.cache_data(show_spinner=False)
def split_words_into_sublists(word_list, max_words_per_list):
    """
    Joins words in a list together and splits them into sublists with a maximum word count
    of `max_words_per_list`.

    Args:
        word_list (list): List of words.
        max_words_per_list (int): Maximum word count per sublist.

    Returns:
        list: List of sublists containing words.
    """
    # Join words into a single string
    joined_words = ' '.join(word_list)

    # Split words into sublists of max_words_per_list each
    sublists = [joined_words[i:i + max_words_per_list] for i in range(0, len(joined_words), max_words_per_list)]

    return sublists

@st.cache_data(show_spinner=False)
def explain_result(query_recommendation, new_question, dataframe_new):

    print("len(dataframe_new.to_json()): ", len(dataframe_new.to_json()))
    ratio_character = len(dataframe_new.to_json())/ 3200
    is_modulo = len(dataframe_new.to_json()) % 3200 > 0
    print(len(dataframe_new.to_json()) % 3200 > 0 )
    if ratio_character < 1:
        batch_size = 1
    else:
        batch_size = round(ratio_character + is_modulo)
    print(f"Batch size: {batch_size}")
    list_of_df = np.array_split(dataframe_new, batch_size)
    # sample data to first 10 dataframe to get result, to remove in prod
    list_of_df = list_of_df[:3]
    list_of_result = []
    for col, dtype in dataframe_new.dtypes.items():
        if 'datetime' in str(dtype):
            dataframe_new[col] = dataframe_new[col].dt.strftime('%Y-%m-%d')
            dataframe_new = dataframe_new.sort_values(by=[col])

    response = recursion_batch(list_of_df, list_of_result, new_question, query_recommendation)

    if response:
        list_of_result_response = []
        st.success('Done!')
        if len(response) >= 2:
            # print("Processing sub explaination")
            max_words_per_list = 3500
            sublists = split_words_into_sublists(response, max_words_per_list)
            # print("Sublist of result: ", sublists)
            response = recursive_summarizer_sub(sublists, list_of_result_response, new_question)
            response = '\n'.join(response)
        else:
            # print("Combining the response")
            response = '\n'.join(response)

    return response




